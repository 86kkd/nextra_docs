## On-policy vs Off-policy

|          |                      On-policy                       |                       Off-policy                       |
| :------: | :--------------------------------------------------: | :----------------------------------------------------: |
| **优点** |       实现更简单<br>训练更稳定<br>理论保证更强       | 更好的样本效率<br>可以重用历史数据<br>可以从演示中学习 |
| **缺点** | 样本效率低<br>需要频繁收集新数据<br>不能重用历史数据 |  实现更复杂<br>训练可能不稳定<br>需要更多的超参数调整  |
